{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_emoji.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miikargh/transformer-emojis/blob/master/notebooks/train_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jPMWK_cGx8i",
        "colab_type": "text"
      },
      "source": [
        "# Training the transformer emoji predictor with a GPU\n",
        "This notebooks shows how to use Google Colab Notebooks to fine-tune a transformer to predict emojis found in tweets. You can find the repo for the code [here](https://github.com/miikargh/transformer-emojis).\n",
        "\n",
        "## Save the dataset to drive\n",
        "First you need to create a dataset to train the model. You can do it by using the `create_dataset.py` script found in the [repo](https://github.com/miikargh/transformer-emojis). Once you have created the dataset you need to split it to train, dev, and test sets by using the `split_dataset.py` script from the repo above.\n",
        "Then all you need to do is save the dataset to your Google Drive and you are good to go üëç"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaTVP0C_wt0L",
        "colab_type": "text"
      },
      "source": [
        "Mount the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D608EZVsGzv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6TlAhckHTbM",
        "colab_type": "text"
      },
      "source": [
        "Check that the dataset exists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkmQXU_yHVIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls /content/drive/My\\ Drive/datasets/tweet_emoji_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGJ4VfdPNBIp",
        "colab_type": "text"
      },
      "source": [
        "Clone the code repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K1sZCwrNEWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/miikargh/transformer-emojis.git && cd transformer-emojis && pip install -r requirements.txt && \\\n",
        "cp -r /content/drive/My\\ Drive/datasets/tweet_emoji_dataset ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBQ8koK0l_9D",
        "colab_type": "text"
      },
      "source": [
        "Run training script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLUcaz30mBoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd transformer-emojis && \\\n",
        "export DATA_DIR=./tweet_emoji_dataset && \\\n",
        "export TASK_NAME=emoji && \\\n",
        "python run_train.py \\\n",
        "    --model_type distilbert \\\n",
        "    --model_name_or_path distilbert-base-multilingual-cased \\\n",
        "    --task_name $TASK_NAME \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_lower_case \\\n",
        "    --data_dir \"${DATA_DIR}\" \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_eval_batch_size=8   \\\n",
        "    --per_gpu_train_batch_size=8   \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --output_dir /tmp/$TASK_NAME/\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}